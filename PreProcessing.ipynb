{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f1iso8ia0RiV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eNrWloVC2NZO"
   },
   "outputs": [],
   "source": [
    "def countWords(local_df):\n",
    "  local_df[\"Word_Count\"] = local_df['Sentences'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/PrynkaSxna/dataset/main/Bhaav-Dataset.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df[\"Sentences\"] = df[\"Sentences\"].str.replace('\\u002C','')\n",
    "df[\"Sentences\"] = df[\"Sentences\"].str.replace('\\u002D','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xH7cSehb5Nsr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayush.garg/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['अद', 'अपन', 'अबक', 'अभ', 'आग', 'आद', 'आपक', 'इत', 'इतय', 'इनक', 'इसक', 'इसम', 'उनक', 'उसक', 'एव', 'ऐस', 'करत', 'करन', 'कह', 'कहत', 'गय', 'जह', 'तन', 'तर', 'दब', 'दर', 'दव', 'धर', 'नन', 'नस', 'नह', 'पहल', 'बत', 'बन', 'बह', 'बड़', 'यत', 'यद', 'रख', 'रत', 'रव', 'रह', 'लक', 'लकर', 'वग', 'वर', 'वग़', 'सक', 'सकत', 'सबस', 'सभ', 'सर'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "hindi_stopwords = ['तुम','मेरी','मुझे','क्योंकि','हम','प्रति','अबकी','आगे','माननीय','शहर','बताएं','कौनसी','क्लिक','किसकी','बड़े','मैं','and','रही','आज','लें','आपके','मिलकर','सब','मेरे','जी','श्री','वैसा','आपका','अंदर', 'अत', 'अपना', 'अपनी', 'अपने', 'अभी', 'आदि', 'आप', 'इत्यादि', 'इन', 'इनका', 'इन्हीं', 'इन्हें', 'इन्हों', 'इस', 'इसका', 'इसकी', 'इसके', 'इसमें', 'इसी', 'इसे', 'उन', 'उनका', 'उनकी', 'उनके', 'उनको', 'उन्हीं', 'उन्हें', 'उन्हों', 'उस', 'उसके', 'उसी', 'उसे', 'एक', 'एवं', 'एस', 'ऐसे', 'और', 'कई', 'कर','करता', 'करते', 'करना', 'करने', 'करें', 'कहते', 'कहा', 'का', 'काफ़ी', 'कि', 'कितना', 'किन्हें', 'किन्हों', 'किया', 'किर', 'किस', 'किसी', 'किसे', 'की', 'कुछ', 'कुल', 'के', 'को', 'कोई', 'कौन', 'कौनसा', 'गया', 'घर', 'जब', 'जहाँ', 'जा', 'जितना', 'जिन', 'जिन्हें', 'जिन्हों', 'जिस', 'जिसे', 'जीधर', 'जैसा', 'जैसे', 'जो', 'तक', 'तब', 'तरह', 'तिन', 'तिन्हें', 'तिन्हों', 'तिस', 'तिसे', 'तो', 'था', 'थी', 'थे', 'दबारा', 'दिया', 'दुसरा', 'दूसरे', 'दो', 'द्वारा', 'न', 'नहीं', 'ना', 'निहायत', 'नीचे', 'ने', 'पर', 'पर', 'पहले', 'पूरा', 'पे', 'फिर', 'बनी', 'बही', 'बहुत', 'बाद', 'बाला', 'बिलकुल', 'भी', 'भीतर', 'मगर', 'मानो', 'मे', 'में', 'यदि', 'यह', 'यहाँ', 'यही', 'या', 'यिह', 'ये', 'रखें', 'रहा', 'रहे', 'ऱ्वासा', 'लिए', 'लिये', 'लेकिन', 'व', 'वर्ग', 'वह', 'वह', 'वहाँ', 'वहीं', 'वाले', 'वुह', 'वे', 'वग़ैरह', 'संग', 'सकता', 'सकते', 'सबसे', 'सभी', 'साथ', 'साबुत', 'साभ', 'सारा', 'से', 'सो', 'ही', 'हुआ', 'हुई', 'हुए', 'है', 'हैं', 'हो', 'होता', 'होती', 'होते', 'होना', 'होने', 'अपनि', 'जेसे', 'होति', 'सभि', 'तिंहों', 'इंहों', 'दवारा', 'इसि', 'किंहें', 'थि', 'उंहों', 'ओर', 'जिंहें', 'वहिं', 'अभि', 'बनि', 'हि', 'उंहिं', 'उंहें', 'हें', 'वगेरह', 'एसे', 'रवासा', 'कोन', 'निचे', 'काफि', 'उसि', 'पुरा', 'भितर', 'हे', 'बहि', 'वहां', 'कोइ', 'यहां', 'जिंहों', 'तिंहें', 'किसि', 'कइ', 'यहि', 'इंहिं', 'जिधर', 'इंहें', 'अदि', 'इतयादि', 'हुइ', 'कोनसा', 'इसकि', 'दुसरे', 'जहां', 'अप', 'किंहों', 'उनकि', 'भि', 'वरग', 'हुअ', 'जेसा', 'नहिं']\n",
    "vectorizer = TfidfVectorizer(stop_words=hindi_stopwords)\n",
    "tfidf = vectorizer.fit_transform(df[\"Sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhdbCtd2dtXF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 fastai-1.0.57 fastprogress-1.0.2 frozenlist-1.3.0 inltk-0.9 multidict-6.0.2 nvidia-ml-py3-7.352.0 pynvx-1.0.0 sentencepiece-0.1.96 typing-3.7.4.3 yarl-1.7.2\n",
      "Found existing installation: torch 1.7.1\n",
      "Uninstalling torch-1.7.1:\n",
      "  Would remove:\n",
      "    /Users/ayush.garg/opt/anaconda3/bin/convert-caffe2-to-onnx\n",
      "    /Users/ayush.garg/opt/anaconda3/bin/convert-onnx-to-caffe2\n",
      "    /Users/ayush.garg/opt/anaconda3/lib/python3.9/site-packages/caffe2/*\n",
      "    /Users/ayush.garg/opt/anaconda3/lib/python3.9/site-packages/torch-1.7.1.dist-info/*\n",
      "    /Users/ayush.garg/opt/anaconda3/lib/python3.9/site-packages/torch/*\n",
      "Proceed (Y/n)? "
     ]
    }
   ],
   "source": [
    "!pip install inltk\n",
    "!pip uninstall torch\n",
    "!pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "from inltk.inltk import setup\n",
    "setup(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9S1VF4SehmX"
   },
   "outputs": [],
   "source": [
    "from inltk.inltk import get_embedding_vectors\n",
    "embeddings = []\n",
    "#RAM exhausted after 200 lines\n",
    "for i in range(0, len(df[\"Sentences\"]), 200):\n",
    "  for j in range(200):\n",
    "    embeddings.append(get_embedding_vectors(df[\"Sentences\"][i+j], \"hi\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "PreProcessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
